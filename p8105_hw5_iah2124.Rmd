---
title: "p8105_hw5_iah2124"
author: "Iris Hart"
date: "2024-11-13"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(purrr)
set.seed(1)
```

## Problem 1 Birthday problem!!!
Let's put people in a room.
```{r}
bday_sim = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n
  return(duplicate)
  
}
bday_sim(10)
```
run this a lot
```{r}
sim_res = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob = mean(res))
sim_res |> 
  ggplot(aes(x = n, y = prob )) + 
  geom_line() + 
  labs(
    title = "Probability of Shared Birthday vs. Group Size",
    x = "Group Size (n)",
    y = "Probability of Shared Birthday"
  ) +
  theme_minimal()
```
With increases group size, the likelihood that at least two people in a group share the same birthday increases. By the time the group size reaches n = 50, there is almost a 100% chance that at least two people have the same birthday. 

Turn this into a function (extra)
```{r}
sim_regression = function(n) {
  
  sim_data = 
    tibble(
      x = rnorm(n, mean = 1, sd = 1),
      y = 2 + 3 * x + rnorm(n, 0, 1)
    )
  lm_fit = lm(y ~ x, data = sim_data)
  out_df = 
    tibble(
      beta0_hat = coef(lm_fit)[1],
      beta1_hat = coef(lm_fit)[2]
    )
  
  return(out_df)
}
sim_res = 
  expand_grid(
    sample_size = c(30, 60), 
    iter = 1:1000
  ) |> 
  mutate(lm_res = map(sample_size, sim_regression)) |> 
  unnest(lm_res)
sim_res |> 
  mutate(sample_size = str_c("n = ", sample_size)) |> 
  ggplot(aes(x = sample_size, y = beta1_hat)) + 
  geom_boxplot() +
    labs(
    title = "Distribution of Beta1 Estimates for Different Sample Sizes",
    x = "Sample Size",
    y = "Beta1 Estimate"
  ) +
  theme_minimal()

sim_res |> 
  filter(sample_size == 30) |> 
  ggplot(aes(x = beta0_hat, y = beta1_hat)) +
  geom_point() + 
  labs(
    title = "Scatter Plot of Beta0 vs Beta1 Estimates for n = 30",
    x = "Beta0 Estimate",
    y = "Beta1 Estimate"
  ) +
  theme_minimal()
```

## Problem 2
Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of ùúá on the x axis. 
```{r}
n <- 30    
sigma <- 5 
alpha <- 0.05  
n_sim <- 5000


simulate_power <- function(mu) {
  p_values <- replicate(n_sim, {
    data <- rnorm(n, mean = mu, sd = sigma)
    t_test <- t.test(data, mu = 0)
    t_test$p.value
  })
  mean(p_values < alpha)
}

mu_values <- 0:6

power_values <- sapply(mu_values, simulate_power)

power_df <- tibble(mu = mu_values, power = power_values)

ggplot(power_df, aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Power of One-Sample t-Test as a Function of True Mean (Effect Size)",
    x = "True Mean (Œº)",
    y = "Power (Proportion of Null Rejections)"
  ) +
  theme_minimal()
```
Describe the association between effect size and power.
As the true mean value increases, the power begisn to increase drastically. By the time the true mean reaches 4, the power levels out to be 1.00, flattening the steep curve upwards. 


Make a plot showing the average estimate of ùúáÃÇ on the y axis and the true value of ùúá on the x axis. Make a second plot (or overlay on the first) the average estimate of ùúáÃÇ only in samples for which the null was rejected on the y axis and the true value of ùúá on the x axis. 
```{r}
simulate_estimates <- function(mu) {
  results <- replicate(n_sim, {
    data <- rnorm(n, mean = mu, sd = sigma)
    t_test <- t.test(data, mu = 0)
    c(sample_mean = mean(data), p_value = t_test$p.value)
  })
 sample_means <- results[1, ]
  p_values <- results[2, ]
  
  avg_sample_mean <- mean(sample_means)

  avg_sample_mean_rejected <- mean(sample_means[p_values < alpha])
  
  c(avg_sample_mean = avg_sample_mean, avg_sample_mean_rejected = avg_sample_mean_rejected)
}

estimates <- sapply(mu_values, simulate_estimates)

estimates_df <- tibble(
  mu = mu_values,
  avg_sample_mean = estimates[1, ],
  avg_sample_mean_rejected = estimates[2, ]
)

ggplot(estimates_df, aes(x = mu)) +
  geom_point(aes(y = avg_sample_mean), color = "coral", size = 3) +
  geom_line(aes(y = avg_sample_mean), color = "coral") +
  geom_point(aes(y = avg_sample_mean_rejected), color = "lightblue", size = 3) +
  geom_line(aes(y = avg_sample_mean_rejected), color = "lightblue") +
  labs(
    title = "Average Estimate of ùúáÃÇ vs. True ùúá",
    x = "True Mean (ùúá)",
    y = "Average Estimate of ùúáÃÇ",
    caption = "Blue: Average ùúáÃÇ across all tests; Red: Average ùúáÃÇ for tests where null was rejected"
  ) +
  theme_minimal()

```
Is the sample average of ùúáÃÇ across tests for which the null is rejected approximately equal to the true value of ùúá? Why or why not?

The average estimated mean for tests where the null hypothesis was rejected deviates from the true value between effect sizes of 0 and 4, with the estimated values being higher than the true value. This occurs because the power to detect a significant difference is lower at smaller effect sizes, meaning a larger difference between the estimated and true values is needed to reject the null hypothesis when the effect size is smaller.

## Problem 3
Loading and viewing homicide data
```{r}
homicide_df <- read_csv("homicide-data.csv") 
summary(homicide_df)
```
This homicide dataset contains 52179 observations with 12 variables including city id, reported date, victim first name and last name, race, age, sex, city, state, latitude, longitude, and disposition.

Creating city state variable and summary
```{r}
homicide_summary = 
  homicide_df |> 
  mutate(city_state = str_c(city, state, sep = ", ")) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) |> 
  ungroup()
```

Baltimore - unsolved homicide proportion 
```{r}
baltimore_df =
  homicide_summary |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    test = list(prop.test(unsolved_homicides, total_homicides) |> 
                  tidy())
  ) |> 
  unnest(test) |> 
  select(estimate,conf.low, conf.high)
baltimore_df
```

Proportion of unsolved homicides in all cities
```{r}
all_cities_df =
  homicide_summary |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = sum(total_homicides),
    unsolved_homicides = sum(unsolved_homicides),
    .groups = "drop"
  ) |> 
  mutate(
    test_results = map2(unsolved_homicides, total_homicides, ~ tidy(prop.test(.x, .y)))
  ) |> 
  unnest(test_results) |> 
  select(city_state, estimate, conf.low, conf.high)

all_cities_df
```

Creating estimate plot with each city 
```{r}
all_cities_df |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = city_state, y = estimate, color = city_state)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.25, color = "royalblue") +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```